.rn '' }`
''' $RCSfile$$Revision$$Date$
'''
''' $Log$
'''
.de Sh
.br
.if t .Sp
.ne 5
.PP
\fB\\$1\fR
.PP
..
.de Sp
.if t .sp .5v
.if n .sp
..
.de Ip
.br
.ie \\n(.$>=3 .ne \\$3
.el .ne 3
.IP "\\$1" \\$2
..
.de Vb
.ft CW
.nf
.ne \\$1
..
.de Ve
.ft R

.fi
..
'''
'''
'''     Set up \*(-- to give an unbreakable dash;
'''     string Tr holds user defined translation string.
'''     Bell System Logo is used as a dummy character.
'''
.tr \(*W-|\(bv\*(Tr
.ie n \{\
.ds -- \(*W-
.ds PI pi
.if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\" diablo 12 pitch
.ds L" ""
.ds R" ""
.ds L' '
.ds R' '
'br\}
.el\{\
.ds -- \(em\|
.tr \*(Tr
.ds L" ``
.ds R" ''
.ds L' `
.ds R' '
.ds PI \(*p
'br\}
.\"	If the F register is turned on, we'll generate
.\"	index entries out stderr for the following things:
.\"		TH	Title 
.\"		SH	Header
.\"		Sh	Subsection 
.\"		Ip	Item
.\"		X<>	Xref  (embedded
.\"	Of course, you have to process the output yourself
.\"	in some meaninful fashion.
.if \nF \{
.de IX
.tm Index:\\$1\t\\n%\t"\\$2"
..
.nr % 0
.rr F
.\}
.TH ROBOTRULES 1 "perl 5.003, patch 93" "25/Nov/96" "User Contributed Perl Documentation"
.IX Title "ROBOTRULES 1"
.UC
.IX Name "WWW::RobotsRules - Parse robots.txt files"
.if n .hy 0
.if n .na
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.de CQ          \" put $1 in typewriter font
.ft CW
'if n "\c
'if t \\&\\$1\c
'if n \\&\\$1\c
'if n \&"
\\&\\$2 \\$3 \\$4 \\$5 \\$6 \\$7
'.ft R
..
.\" @(#)ms.acc 1.5 88/02/08 SMI; from UCB 4.2
.	\" AM - accent mark definitions
.bd B 3
.	\" fudge factors for nroff and troff
.if n \{\
.	ds #H 0
.	ds #V .8m
.	ds #F .3m
.	ds #[ \f1
.	ds #] \fP
.\}
.if t \{\
.	ds #H ((1u-(\\\\n(.fu%2u))*.13m)
.	ds #V .6m
.	ds #F 0
.	ds #[ \&
.	ds #] \&
.\}
.	\" simple accents for nroff and troff
.if n \{\
.	ds ' \&
.	ds ` \&
.	ds ^ \&
.	ds , \&
.	ds ~ ~
.	ds ? ?
.	ds ! !
.	ds /
.	ds q
.\}
.if t \{\
.	ds ' \\k:\h'-(\\n(.wu*8/10-\*(#H)'\'\h"|\\n:u"
.	ds ` \\k:\h'-(\\n(.wu*8/10-\*(#H)'\`\h'|\\n:u'
.	ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'^\h'|\\n:u'
.	ds , \\k:\h'-(\\n(.wu*8/10)',\h'|\\n:u'
.	ds ~ \\k:\h'-(\\n(.wu-\*(#H-.1m)'~\h'|\\n:u'
.	ds ? \s-2c\h'-\w'c'u*7/10'\u\h'\*(#H'\zi\d\s+2\h'\w'c'u*8/10'
.	ds ! \s-2\(or\s+2\h'-\w'\(or'u'\v'-.8m'.\v'.8m'
.	ds / \\k:\h'-(\\n(.wu*8/10-\*(#H)'\z\(sl\h'|\\n:u'
.	ds q o\h'-\w'o'u*8/10'\s-4\v'.4m'\z\(*i\v'-.4m'\s+4\h'\w'o'u*8/10'
.\}
.	\" troff and (daisy-wheel) nroff accents
.ds : \\k:\h'-(\\n(.wu*8/10-\*(#H+.1m+\*(#F)'\v'-\*(#V'\z.\h'.2m+\*(#F'.\h'|\\n:u'\v'\*(#V'
.ds 8 \h'\*(#H'\(*b\h'-\*(#H'
.ds v \\k:\h'-(\\n(.wu*9/10-\*(#H)'\v'-\*(#V'\*(#[\s-4v\s0\v'\*(#V'\h'|\\n:u'\*(#]
.ds _ \\k:\h'-(\\n(.wu*9/10-\*(#H+(\*(#F*2/3))'\v'-.4m'\z\(hy\v'.4m'\h'|\\n:u'
.ds . \\k:\h'-(\\n(.wu*8/10)'\v'\*(#V*4/10'\z.\v'-\*(#V*4/10'\h'|\\n:u'
.ds 3 \*(#[\v'.2m'\s-2\&3\s0\v'-.2m'\*(#]
.ds o \\k:\h'-(\\n(.wu+\w'\(de'u-\*(#H)/2u'\v'-.3n'\*(#[\z\(de\v'.3n'\h'|\\n:u'\*(#]
.ds d- \h'\*(#H'\(pd\h'-\w'~'u'\v'-.25m'\f2\(hy\fP\v'.25m'\h'-\*(#H'
.ds D- D\\k:\h'-\w'D'u'\v'-.11m'\z\(hy\v'.11m'\h'|\\n:u'
.ds th \*(#[\v'.3m'\s+1I\s-1\v'-.3m'\h'-(\w'I'u*2/3)'\s-1o\s+1\*(#]
.ds Th \*(#[\s+2I\s-2\h'-\w'I'u*3/5'\v'-.3m'o\v'.3m'\*(#]
.ds ae a\h'-(\w'a'u*4/10)'e
.ds Ae A\h'-(\w'A'u*4/10)'E
.ds oe o\h'-(\w'o'u*4/10)'e
.ds Oe O\h'-(\w'O'u*4/10)'E
.	\" corrections for vroff
.if v .ds ~ \\k:\h'-(\\n(.wu*9/10-\*(#H)'\s-2\u~\d\s+2\h'|\\n:u'
.if v .ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'\v'-.4m'^\v'.4m'\h'|\\n:u'
.	\" for low resolution devices (crt and lpr)
.if \n(.H>23 .if \n(.V>19 \
\{\
.	ds : e
.	ds 8 ss
.	ds v \h'-1'\o'\(aa\(ga'
.	ds _ \h'-1'^
.	ds . \h'-1'.
.	ds 3 3
.	ds o a
.	ds d- d\h'-1'\(ga
.	ds D- D\h'-1'\(hy
.	ds th \o'bp'
.	ds Th \o'LP'
.	ds ae ae
.	ds Ae AE
.	ds oe oe
.	ds Oe OE
.\}
.rm #[ #] #H #V #F C
.SH "NAME"
.IX Header "NAME"
WWW::RobotsRules \- Parse robots.txt files
.SH "SYNOPSIS"
.IX Header "SYNOPSIS"
.PP
.Vb 2
\& require WWW::RobotRules;
\& my $robotsrules = new WWW::RobotRules 'MOMspider/1.0';
.Ve
.Vb 1
\& use LWP::Simple qw(get);
.Ve
.Vb 3
\& $url = "http://some.place/robots.txt";
\& my $robots_txt = get $url;
\& $robotsrules->parse($url, $robots_txt);
.Ve
.Vb 3
\& $url = "http://some.other.place/robots.txt";
\& my $robots_txt = get $url;
\& $robotsrules->parse($url, $robots_txt);
.Ve
.Vb 6
\& # Now we are able to check if a URL is valid for those servers that
\& # we have obtained and parsed "robots.txt" files for.
\& if($robotsrules->allowed($url)) {
\&     $c = get $url;
\&     ...
\& }
.Ve
.SH "DESCRIPTION"
.IX Header "DESCRIPTION"
This module parses a \fIrobots.txt\fR file as specified in
\*(L"A Standard for Robot Exclusion\*(R", described in
<URL:http://info.webcrawler.com/mak/projects/robots/norobots.html>
Webmasters can use the \fIrobots.txt\fR file to disallow conforming
robots access to parts of their WWW server.
.PP
The parsed file is kept in the WWW::RobotRules object, and this object
provide methods to check if access to a given URL is prohibited.  The
same WWW::RobotRules object can parse multiple \fIrobots.txt\fR files.
.SH "METHODS"
.IX Header "METHODS"
.Sh "\f(CW$rules\fR = new \s-1WWW::\s0RobotRules \*(L'MOMspider/1.0\*(R'"
.IX Subsection "\f(CW$rules\fR = new \s-1WWW::\s0RobotRules \*(L'MOMspider/1.0\*(R'"
This is the constructor for \s-1WWW::\s0RobotRules objects.  The first 
argument given to \fInew()\fR is the name of the robot. 
.Sh "\f(CW$rules\fR\->\fIparse\fR\|($url, \f(CW$content\fR, \f(CW$fresh_until\fR)"
.IX Subsection "\f(CW$rules\fR\->\fIparse\fR\|($url, \f(CW$content\fR, \f(CW$fresh_until\fR)"
The \fIparse()\fR method takes as arguments the \s-1URL\s0 that was used to
retrieve the \fI/robots.txt\fR file, and the contents of the file.
.Sh "\f(CW$rules\fR\->\fIallowed\fR\|($url)"
.IX Subsection "\f(CW$rules\fR\->\fIallowed\fR\|($url)"
Returns \s-1TRUE\s0 if this robot is allowed to retrieve this \s-1URL\s0.
.Sh "\f(CW$rules\fR\->\fIagent\fR\|([$name])"
.IX Subsection "\f(CW$rules\fR\->\fIagent\fR\|([$name])"
Get/set the agent name. \s-1NOTE\s0: Changing the agent name will clear the robots.txt
rules and expire times out of the cache.
.SH "ROBOTS.TXT"
.IX Header "ROBOTS.TXT"
The format and semantics of the \*(L"/robots.txt\*(R" file are as follows
(this is an edited abstract of
<URL:http://info.webcrawler.com/mak/projects/robots/norobots.html>):
.PP
The file consists of one or more records separated by one or more
blank lines. Each record contains lines of the form
.PP
.Vb 1
\&  <field-name>: <value>
.Ve
The field name is case insensitive.  Text after the \*(L'#\*(R' character on a
line is ignored during parsing.  This is used for comments.  The
following <field-names> can be used:
.Ip "User-Agent" 3
.IX Item "User-Agent"
The value of this field is the name of the robot the record is
describing access policy for.  If more than one \fIUser-Agent\fR field is
present the record describes an identical access policy for more than
one robot. At least one field needs to be present per record.  If the
value is \*(L'*\*(R', the record describes the default access policy for any
robot that has not not matched any of the other records.
.Ip "Disallow" 3
.IX Item "Disallow"
The value of this field specifies a partial \s-1URL\s0 that is not to be
visited. This can be a full path, or a partial path; any \s-1URL\s0 that
starts with this value will not be retrieved
.Sh "Examples"
.IX Subsection "Examples"
The following example \*(L"/robots.txt\*(R" file specifies that no robots
should visit any \s-1URL\s0 starting with \*(L"/cyberworld/map/\*(R" or \*(L"/tmp/":
.PP
.Vb 1
\&  # robots.txt for http://www.site.com/
.Ve
.Vb 3
\&  User-agent: *
\&  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
\&  Disallow: /tmp/ # these will soon disappear
.Ve
This example \*(L"/robots.txt\*(R" file specifies that no robots should visit
any \s-1URL\s0 starting with \*(L"/cyberworld/map/\*(R", except the robot called
\*(L"cybermapper":
.PP
.Vb 1
\&  # robots.txt for http://www.site.com/
.Ve
.Vb 2
\&  User-agent: *
\&  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
.Ve
.Vb 3
\&  # Cybermapper knows where to go.
\&  User-agent: cybermapper
\&  Disallow:
.Ve
This example indicates that no robots should visit this site further:
.PP
.Vb 3
\&  # go away
\&  User-agent: *
\&  Disallow: /
.Ve
.SH "SEE ALSO"
.IX Header "SEE ALSO"
the \fILWP::RobotUA\fR manpage, the \fIWWW::RobotRules::AnyDBM_File\fR manpage

.rn }` ''
